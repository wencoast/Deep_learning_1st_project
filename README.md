# Deep_learning_1st_project
Simple neural network and ResNet performance comparison

## Running Environment

<ul style="list-style: none;">
  
  <li>python 3.6.7</li>
  <li>Tensorflow 1.12.0</li>
  <li>keras 2.1.6-tf</li>

</ul>

## Running the Code for SNN

| Path          | Function      |
| ------------- |---------------|
| SNN/snn_fashion_mnist.py | The original structure and hyperparameters |
| SNN/snn_fashion_mnist_lr_optimizer.py | Compare Momentum and Adam with different learning rates |
| SNN/snn_fashion_mnist_dropout.py | Compare different dropout rates |
| SNN/snn_fashion_mnist_optimal.py|           The final model with optimal settings|


## TODO List

<ol>
  
  <li>Comparison of Deep network architectures Simple neural network and ResNet.[Done!]</li>
  <li>Using Momentum optimizer and Adam optimizer with different learning rate.[Done!]</li>
  <li>Using dropout.[Done!]</li>
  <li>Using batch normalization.[Done!]</li>
  <li>Using different activation functions including relu, tanh, leaky_relu, Sigmoid, etc. [Done!] </li>
  <li>Using data augmentation.[Done!]</li>
  <li>Using different optimizers such as ADAGRAD, ADADELTA, ADAM, RMSPROP, MOM. [Done!]</li>
  <li>Using local response normalization.[Done!]</li>
  
</ol>
